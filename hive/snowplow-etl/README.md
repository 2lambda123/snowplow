# SnowPlow::Etl

## Introduction

SnowPlow::Etl is a Ruby gem (built with [Bundler] [bundler]) to run the daily
ETL (extract, transform, load) job which processes the CloudFront access logs
generated by `snowplow.js` and turns them into SnowPlow-format, 
day-partitioned event files, all stored in Amazon S3.

Currently the SnowPlow::Etl gem performs the ETL process on Amazon Elastic 
MapReduce using Hive. A future version of this gem will offer a second ETL 
option using a "vanilla Hadoop" process on EMR.

## Installation

### Dependencies

To make use of SnowPlow::Etl you will need the following:

1. An [Amazon Web Services] [aws] account
2. SnowPlow tracking successfully implemented on your website. Please see the
   [Tracker Setup Guide] [trackerguide] for help implementing this
3. Git installed. Please see the [Git Installation Guide] [gitguide] as needed  
3. RubyGems installed. Please see the [RubyGems Installation Instructions] [gemsguide]
   as needed
4. Bundler (a Ruby gem) installed:

    $ gem install bundler

### Installation

First checkout the repository:

    $ xxx
    
Now install the SnowPlow::ETL gem on your system:

    $ cd xxx
    $ gem install snowplow-etl

Now test that the gem was installed successfully:

    $ bundle exec snowplow-etl --version
    snowplow-etl 0.0.1

### Configuration

SnowPlow::Etl requires a YAML format configuration file to run. There
is a configuration file template available in the repository at 
`hive/snowplow-etl/config/config.yml`. The contents should be as
follows:

    :aws:
      :access_key_id: ADD HERE
      :secret_access_key: ADD HERE
    :buckets:
      :query: ADD HERE
      :serde: ADD HERE
      :in: ADD HERE
      :out: ADD HERE
      :archive: ADD HERE

The `aws` variables should be self-explanatory. The `buckets` variables
are as follows:

* `query` is the bucket to install the ETL job's HiveQL scripts
* `serde` is the bucket to install the ETL job's Hive deserializer
* `in` is the bucket containing the CloudFront access logs to process
* `out` is the bucket to store the SnowPlow-format event files in
* `archive` is the bucket to move the processed CloudFront logs to

All `buckets` variables can include a sub-folder within the bucket as 
required. A trailing slash is optional. For example, the following are
all valid configuration settings:

    :buckets:
      :query: my-snowplow-static/hiveql/
      :serde: my-snowplow-static/jars
      :in: my-snowplow-log-bucket

## Usage

### Overview

There are two usage modes for 

TODO: Write usage instructions here 

## Contributing

1. Fork it
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Added some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create new Pull Request

[bundler]: xxx
[aws]: xxx
[trackerguide]: xxx
[gitguide]: xxx
[gemsguide]: xxx
