# SnowPlow::Etl

## Introduction

SnowPlow::Etl is a Ruby gem (built with [Bundler] [bundler]) to run the daily
ETL (extract, transform, load) job which processes the CloudFront access logs
generated by `snowplow.js` and turns them into SnowPlow-format, 
day-partitioned event files, all stored in Amazon S3.

Currently the SnowPlow::Etl gem performs the ETL process on Amazon Elastic 
MapReduce using Hive. A future version of this gem will offer a second ETL 
option using a "vanilla Hadoop" process on EMR. If you are interested in this,
please vote for [this issue] [hadoopetl].

## Installation

### Dependencies

To make use of SnowPlow::Etl you will need the following:

1. An [Amazon Web Services] [aws] account
2. SnowPlow tracking successfully implemented on your website. Please see the
   [Tracker Setup Guide] [trackerguide] for help implementing this
3. Git installed. Please see the [Git Installation Guide] [gitguide] as needed  
4. RubyGems installed. Please see the [RubyGems Installation Instructions] [gemsguide]
   as needed
5. Bundler (a Ruby gem) installed:  

    $ gem install bundler

### Installation

First checkout the repository:

    $ xxx
    
Now install the SnowPlow::ETL gem on your system:

    $ cd xxx
    $ gem install snowplow-etl

Now test that the gem was installed successfully:

    $ bundle exec snowplow-etl --version
    snowplow-etl 0.0.1

### Configuration

SnowPlow::Etl requires a YAML format configuration file to run. There
is a configuration file template available in the repository at 
`hive/snowplow-etl/config/config.yml`. The contents should be as
follows:

    :aws:
      :access_key_id: ADD HERE
      :secret_access_key: ADD HERE
    :buckets:
      :query: ADD HERE
      :serde: ADD HERE
      :in: ADD HERE
      :out: ADD HERE
      :archive: ADD HERE

The `aws` variables should be self-explanatory. The `buckets` variables
are as follows:

* `query` is the bucket to install the ETL job's HiveQL scripts
* `serde` is the bucket to install the ETL job's Hive deserializer
* `in` is the bucket containing the CloudFront access logs to process
* `out` is the bucket to store the SnowPlow-format event files in
* `archive` is the bucket to move the processed CloudFront logs to

All `buckets` variables can include a sub-folder within the bucket as 
required. A trailing slash is optional. For example, the following are
all valid configuration settings:

    :buckets:
      :query: my-snowplow-static/hiveql/
      :serde: my-snowplow-static/jars
      :in: my-snowplow-log-bucket

Please note that all buckets must exist prior to running SnowPlow::Etl,
and currently the `query`, `serde` and `archive` buckets must be in a 
US data center (not in e.g. Europe). This latter point is on account of 
[this bug] [s3bug].

## Usage

### Overview

There are two usage modes for the SnowPlow::Etl gem:

1. **Daily mode** where the gem is run daily to process the last 24 hours
   worth of CloudFront access logs ready for SnowPlow
2. **Catchup mode** where the gem is run across a "datespan" of multiple 
   days to bring processing of the CloudFront access logs up-to-date 

In particular, catchup mode is useful when running SnowPlow::Etl for the 
first time, or when something has gone wrong with daily mode and a day's
processing needs to be rerun.

### Command-line options

The command-line options for SnowPlow::Etl look like this:

    Usage: snowplow-etl [options]

    Specific options:
        -c, --config CONFIG              configuration file
        -s, --start YYYY-MM-DD           start date (defaults to yesterday)
        -e, --end YYYY-MM-DD             end date (defaults to yesterday)

    Common options:
        -h, --help                       Show this message
        -v, --version                    Show version
   
Invoking SnowPlow::Etl with just a `--config` option puts it into daily
mode, with yesterday as the day being processed:

    $ bundle exec snowplow-etl --config my-config.yml
 
To run SnowPlow::Etl in catchup mode, simply specify the start and end
dates to use:

    $ bundle exec snowplow-etl --config my-config.yml --start 2012-06-20 --end 2012-06-24 

This will run SnowPlow::Etl for the period 20 June 2012 to 24 June 2012
inclusive.

### Scheduling

Once you have the ETL process working smoothly, you can set up a daily cronjob
to automate the daily ETL process. The job should run in the early morning, 
when the full set of CloudFront log files for yesterday have been finalised.

Assuming that you have the excellent [cronic] [cronic] installed, you can
setup your cronjob like so:

    $ TODO

[bundler]: xxx
[hadoopetl]: xxx
[aws]: xxx
[trackerguide]: xxx
[gitguide]: xxx
[gemsguide]: xxx
[cronic]: xxx
[s3bug]: xxx
